# -*- coding: utf-8 -*-
"""linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dSwJGtzoxAFZlW8itoyHW0NtJSze1Pk7
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from jax import grad
import jax
from sklearn.linear_model import LinearRegression as LR
from sklearn.metrics import mean_absolute_error, mean_squared_error
import jax.numpy as jnp

np.random.seed(45)

class LinearRegression():
  def __init__(self, fit_intercept = True ):
    # Initialize relevant variables
    '''
        :param fit_intercept: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).
    '''
    self.fit_intercept = fit_intercept 
    self.coef_ = None #Replace with numpy array or pandas series of coefficients learned using using the fit methods
    self.all_coef=pd.DataFrame([]) # Stores the thetas for every iteration (theta vectors appended) (for the iterative methods)
    #self.x = None 
    #self.y = None 
    self.X_batch = None
    self.y_batch = None 
    self.coef_ = None
    #self.intercept_ = None

    self.intercept_ = 0
  

  def fit_sklearn_LR(self,x,y):
    # Solve the linear regression problem by calling Linear Regression
    # from sklearn, with the relevant parameters
    self.x = x
    self.y = y
    self.n_samples, self.n_features = x.shape
    model = LR().fit(x,y)
    self.coef_ = model.coef_
    # self.intercept_ = model.intercept_
    if self.fit_intercept:
       self.coef_ = np.insert(self.coef_, 0, model.intercept_, axis = 0)
    
  def fit_normal_equations(self,x,y):
    # Solve the linear regression problem using the closed form solution
    # to the normal equation for minimizing ||Wx - y||_2^2
    if self.fit_intercept:
      ones = np.ones(len(x)).reshape(len(x),1)
      x = np.concatenate((ones, x), axis=1)
    x_transpose = x.T
    self.coef_ = np.dot(np.dot(np.linalg.inv(np.dot(x_transpose, x)), x_transpose), y)
    
    

  def fit_SVD(self,x,y):
    # Solve the linear regression problem using the SVD of the 
    # coefficient matrix
    if self.fit_intercept:
      ones = np.ones(len(x)).reshape(len(x),1)
      x = np.concatenate((ones, x), axis=1)
    u, s, vh = np.linalg.svd(x, full_matrices=False)
    s_inv = np.diag(1 / s)
    x_pinv = np.dot(vh.T, np.dot(s_inv, u.T))
    self.coef_ = np.dot(x_pinv, y)
    
  print("start of the question number2.") 
    ##########################################################################################################################
  def mse_loss(self,X,y):                
    # Compute the MSE loss with the learned model
    # Compute the predicted y values using the learned model
    #y_pred = self.predict(x)
    # Compute the mean squared error
    #mse = ((y_pred - y)**2).mean()
    #return mse
    y_hat = np.dot(X, self.coef_)
    return np.mean((y_hat - y)**2)

  def compute_gradient(self,X,y, penalty_type, penalty_value):
    # Compute the analytical gradient (in vectorized form) of the 
    # 1. unregularized mse_loss,  and 
    # 2. mse_loss with ridge regularization
    # penalty :  specifies the regularization used  , 'l2' or unregularized
    #y_pred = self.predict(x)
    #error = y_pred - self.y
    #grad_unreg = (2 / self.n_samples) * np.dot(self.x.T, error)
    y_hat = np.dot(X, self.coef_)
    y = y.reshape(X.shape[0],)
    n,d = X.shape





    if penalty_type == 'l2':
      # gradient of mse loss with ridge regularization
      # print('gradinet before',(2/n) * X.T.dot(y_hat - y))
      gradient = (2/n) * X.T.dot(y_hat - y) + 2 *penalty_value* self.coef_
      # print('gradient after ',gradient)
      # print('penalty value is ',penalty_value)
      # print('in l2')
    else:
      # gradient of unregularized mse loss
      gradient = (2/n) * X.T.dot(y_hat - y)
    a = X.T.dot(y_hat - y)

    return gradient






















    '''
    if penalty == 'l2':
        # Ridge regularization gradient
        grad_reg = grad_unreg + 2 * self.alpha * self.coef_
        return grad_reg
    elif penalty == 'l1':
    # Lasso regularization gradient
      grad_reg = grad_unreg + self.alpha * np.sign(self.coef_)
      return grad_reg
    else:
        # No regularization
        return grad_unreg
        '''

  def compute_jax_gradient(self,X,y, penalty_type, penalty_value):
    # Compute the gradient of the 
    # 1. unregularized mse_loss, 
    # 2. mse_loss with LASSO regularization and 
    # 3. mse_loss with ridge regularization, using JAX 
    # penalty :  specifies the regularization used , 'l1' , 'l2' or unregularized
    # Define the loss function
    '''def loss_fn(params, x, y):
        y_pred = self.predict(x, params)
        mse_loss = jnp.mean(jnp.square(y_pred - y))
        if penalty == 'l1':
            l1_penalty = alpha * jnp.sum(jnp.abs(params))
            return mse_loss + l1_penalty
        elif penalty == 'l2':
            l2_penalty = alpha * jnp.sum(jnp.square(params))
            return mse_loss + l2_penalty
        else:
            return mse_loss
        # Define a function to compute the gradients of the loss function using JAX
    grad_fn = jax.grad(loss_fn)

    # Compute the gradients of the loss function for the given penalty
    if penalty == 'l1':
        params_grad = grad_fn(self.params, self.x, self.y)
    elif penalty == 'l2':
        params_grad = grad_fn(self.params, self.x, self.y)
    else:
        params_grad = grad_fn(self.params, self.x, self.y)

    return params_grad
    '''
    X = np.array(X)
    y = np.array(y)
    y = y.reshape(X.shape[0],)
    
    def mse_loss_jax1(theta):
      y_hat = jnp.dot(X.astype('float'), theta)
      loss = jnp.mean((y_hat - y)**2)
      return loss    
    gradient_func = jax.grad(mse_loss_jax1, argnums=0)
    
    gradient = gradient_func(self.coef_)
    if penalty_type == 'l1':
      # gradient of mse loss with LASSO regularization
      gradient = gradient + penalty_value * np.sign(self.coef_)
    elif penalty_type == 'l2':
      # gradient of mse loss with ridge regularization
      # print('grad before ',gradient)
      gradient = gradient + 2 * penalty_value * self.coef_
      # print('grad after ',gradient)
      # print('penalty value is ',penalty_value)
      # print('in l2 jax')
    else:
      # gradient of unregularized mse loss
      gradient = gradient
    return gradient

  def fit_gradient_descent(self,X,y, batch_size, gradient_type, penalty_type,penalty_value=0, num_iters=20, lr=0.01):
    # Implement batch gradient descent for linear regression (should unregularized as well as 'l1' and 'l2' regularized objective)
    # batch_size : Number of training points in each batch
    # num_iters : Number of iterations of gradient descent
    # lr : Default learning rate
    # gradient_type : manual or JAX gradients
    # penalty_type : 'l1', 'l2' or unregularized
        # Split data into batches
    # # self.x = x
    # # self.y = y
    # self.n_samples, self.n_features = x.shape
    # self.coef_ = np.zeros((self.n_features, 1))
    # num_batches = int(np.ceil(self.n_samples / batch_size))
    # batches_X = np.array_split(self.x, num_batches)
    # batches_y = np.array_split(self.y, num_batches)
    '''
    num_batches = int(np.ceil(x.shape[0] / batch_size))
    batches_X = np.array_split(x, num_batches)
    batches_y = np.array_split(y, num_batches)
    # Initialize coefficients
    self.coef_ = np.zeros(x.shape[1])

    # Define the penalty
    if penalty_type == 'l1':
        penalty = 'l1'
    elif penalty_type == 'l2':
        penalty = 'l2'
    else:
        penalty = 'unregularized'

    # Define the gradient function
    if gradient_type == 'manual':
        grad_fn = self.compute_gradient(x,y, penalty, penalty_value)
    elif gradient_type == 'jax':
        grad_fn = self.compute_jax_gradient(x,y, penalty)
    else:
        raise ValueError('Invalid gradient_type parameter')

    # Perform gradient descent
    for i in range(num_iters):
        for j in range(num_batches):
            batch_X = batches_X[j]
            batch_y = batches_y[j]
            grad = grad_fn(penalty=penalty)
            
            self.coef_ -= lr * grad

    # Return the fitted coefficients
    return self.coef_
    '''
    if self.fit_intercept==True:
      # X = np.hstack((np.ones((X.shape[0], 1)), X))
      ones = np.ones(len(X)).reshape(len(X),1)
      X = np.concatenate((ones, X), axis=1)

    #----------------
    # batch code final
    
    # print('we are in fit_gradient_descent')
    mini_batches = []
    y = np.array(y)
    y = y.reshape((-1,1))
    
    data = np.hstack((X, y))
    # np.random.shuffle(data) # -------------------------------------------shuffle the data
    print('--------')
    n_minibatches = data.shape[0] // batch_size
    i = 0

    for i in range(n_minibatches):
        mini_batch = data[i * batch_size:(i + 1)*batch_size, :]
        X_mini = mini_batch[:, :-1]
        Y_mini = mini_batch[:, -1].reshape((-1, 1))
        mini_batches.append((X_mini, Y_mini))

    if data.shape[0] - (i+1)*batch_size:
        mini_batch = data[(i+1)*batch_size: , :]

        X_mini = mini_batch[:, :-1]
        Y_mini = mini_batch[:, -1].reshape((-1, 1))
        mini_batches.append((X_mini, Y_mini))
    # --------------------
    np.random.seed(45)
    # self.coef_ = np.random.rand(X.shape[1]) # initialize the coefficients
    # self.coef_ = np.ones(X.shape[1]+1) # initialize the coefficients
    self.coef_ = np.zeros(X.shape[1]) # initialize the coefficients
    # self.all_coef = []
    # print('initial coef ',self.coef_)
    for itr in range(num_iters):
      for mini_batch in mini_batches:
        X_mini, y_mini = mini_batch
        # print(X_mini.shape, y_mini.shape,'shape')
        if gradient_type == 'manual':
          gradient = self.compute_gradient(X_mini,y_mini,penalty_type, penalty_value)
        else:
          gradient = self.compute_jax_gradient(X_mini,y_mini,penalty_type, penalty_value)
        # print(gradient,'++++++++++' )
        # print(self.coef_.shape, gradient.shape)
        self.coef_ = self.coef_ - lr * gradient
        # input
        # if itr%50 == 0:
        #   input(print(self.coef_))
        # self.all_coef.append(self.coef_)
        # self.all_coef = self.all_coef.append(pd.Series(self.coef_), ignore_index=True)
    # print(len(self.all_coef), self.coef_.shape,'------')
    print('final coef ' ,self.coef_)

    # self.all_coef = np.array(self.all_coef)
    # self.all_coef = self.all_coef.reshape(num_iters, n_minibatches+1, X.shape[1])
    # print(self.all_coef.shape, self.coef_.shape,'@@@@@@@@@@@@@')
    # self.all_coef = np.mean(self.all_coef, axis=1)
    # self.all_coef = self.all_coef.reshape(num_iters, X.shape[1])
    # self.coef_ = self.all_coef[-1]
    # self.all_coef = self.all_coef.tolist()

    # print(len(self.all_coef), self.coef_.shape,'------')
    # print(self.all_coef)
    # print(self.coef_)
    # print('allcoef : ',self.all_coef)
    # input()
    return self.coef_






  def fit_SGD_with_momentum(self, penalty='l2', beta=0.9):
    # Solve the linear regression problem using sklearn's implementation of SGD
    # penalty: refers to the type of regularization used (ridge)
    pass

  def predict(self, x):
    # Funtion to run the LinearRegression on a test data point
    if self.fit_intercept:
      ones = np.ones(len(x)).reshape(len(x),1)
      x = np.concatenate((ones, x), axis=1)
    return np.dot(x, self.coef_)
  
  def evaluate(self, x, y):
    y_hat = self.predict(x)
    mae = mean_absolute_error(y, y_hat)
    rmse = mean_squared_error(y, y_hat)
    return mae, rmse
    


  def plot_surface(self, X, y, theta_0, theta_1):
    '''
    Function to plot RSS (residual sum of squares) in 3D. A surface plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1 by a
    red dot. Uses self.coef_ to calculate RSS. Plot must indicate error as the title.
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to indicate RSS #pd Series of all theta_0
      :param theta_1: Value of theta_1 for which to indicate RSS #pd Series of all theta_1
      :return matplotlib figure plotting RSS

    '''
    '''
    print("start of qn3")
    theta_0 = theta_0.ravel()
    theta_1 = theta_1.ravel()
    theta_0_grid, theta_1_grid = np.meshgrid(theta_0, theta_1)
    rss_grid = np.zeros_like(theta_0_grid)
    for i, t0 in enumerate(theta_0):
      for j, t1 in enumerate(theta_1):
        rss_grid[j, i] = np.sum((y - (t0 + t1*X))**2)
    fig = plt.figure()
    ax = fig.gca(projection='3d')
    ax.set_title('RSS')
    ax.set_xlabel('Theta_0')
    ax.set_ylabel('Theta_1')
    ax.set_zlabel('RSS')
    ax.plot_surface(theta_0_grid, theta_1_grid, rss_grid, cmap='plasma')
    ax.scatter(self.coef_[0], self.coef_[1], np.sum((y - self.predict(X))**2), color='r')
    plt.show()  '''





    

  def plot_line_fit(self, X, y, theta_0, theta_1):
    """
    Function to plot fit of the line (y vs. X plot) based on chosen value of theta_0, theta_1. Plot must
    indicate theta_0 and theta_1 as the title.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting line fit
    """
    pass


  def plot_contour(self, X, y, theta_0, theta_1):
    """
    Plots the RSS as a contour plot. A contour plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1, and the
    direction of gradient steps. Uses self.coef_ to calculate RSS.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting the contour
    """
    pass